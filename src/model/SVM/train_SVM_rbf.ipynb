{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM multiclass model with 'rbf' kernel ???\n",
    "Parameters of the model:\n",
    "- $C_{\\text{raw}} = 10^3$\n",
    "- $C_{\\text{scaled}} = 250$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.model.SVM.SVM_module import *\n",
    "from src.train.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels loaded!\n",
      "Example:\n",
      "X: [-2.03834213e+02  1.65272430e+02 -2.71203651e+01  3.98504753e+01\n",
      "  1.52355738e+01  1.52752247e+01 -5.82818985e+00  1.24469271e+01\n",
      " -2.53585100e+00  5.68161488e+00  2.86470144e-02  4.47464501e-03\n",
      "  2.23200000e+03  1.20789442e+03  1.52517064e+03  2.32513772e+01\n",
      "  2.57816840e+01  2.43512780e+01  2.44582548e+01  2.38234885e+01\n",
      "  2.04741587e+01  2.52746605e+01  6.54243573e-04  6.98350232e-01\n",
      "  5.18728181e-01  4.41854394e-01  1.96658298e-01  2.66243911e+00] \n",
      "y: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data = ManageData()\n",
    "# Loading the training features and labels\n",
    "data.LoadTrainFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation features and labels loaded!\n",
      "Example:\n",
      "X: [ 4.24629608e+02  1.57892838e+02 -6.61561127e+01  3.53172836e+01\n",
      " -1.08259525e+01  1.99779091e+01  9.03547859e+00  1.16253910e+01\n",
      " -6.59319687e+00  9.90838528e+00  4.58988983e-02  1.11563884e-01\n",
      "  2.98343434e+03  1.57618474e+03  1.50766778e+03  2.51165358e+01\n",
      "  1.80690291e+01  1.96382736e+01  1.99261764e+01  1.70384129e+01\n",
      "  2.10494327e+01  5.28803644e+01  2.43222667e-03  7.72531224e-01\n",
      "  6.70360129e-01  6.16755698e-01  3.44813168e-01  2.22349119e+00] \n",
      "y: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Loading the validation features and labels\n",
    "data.LoadValidationFeatures()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using non-normalized (raw) features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>multilabelSVM(C=1000.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">multilabelSVM</label><div class=\"sk-toggleable__content\"><pre>multilabelSVM(C=1000.0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "multilabelSVM(C=1000.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model_raw = multilabelSVM(C=1e3, kernel='rbf')\n",
    "model_raw.fit(X=data.X_train, y=data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = '.\\src\\model\\SVM\\saved_models\\SVM_model_rbf_raw.sav'\n",
    "pickle.dump(model_raw, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few predictions of the model\n",
    "prediction_raw = model_raw.predict(X=data.X_val)\n",
    "prediction_raw[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating various metrics for the given model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy                                        tensor(0.8400)\n",
      "hamming_distance                                tensor(0.1600)\n",
      "f1                                              tensor(0.0104)\n",
      "precision                                       tensor(0.0848)\n",
      "recall                                          tensor(0.0056)\n",
      "instruments/cello_accuracy                      tensor(0.9595)\n",
      "instruments/clarinet_accuracy                   tensor(0.9902)\n",
      "instruments/flute_accuracy                      tensor(0.9656)\n",
      "instruments/acoustic_guitar_accuracy            tensor(0.8219)\n",
      "instruments/electric_guitar_accuracy            tensor(0.7039)\n",
      "instruments/organ_accuracy                      tensor(0.8882)\n",
      "instruments/piano_accuracy                      tensor(0.5491)\n",
      "instruments/saxophone_accuracy                  tensor(0.7948)\n",
      "instruments/trumpet_accuracy                    tensor(0.9029)\n",
      "instruments/violin_accuracy                     tensor(0.9287)\n",
      "instruments/human_voice_accuracy                tensor(0.7346)\n",
      "instruments/cello_hamming_distance              tensor(0.0405)\n",
      "instruments/clarinet_hamming_distance           tensor(0.0098)\n",
      "instruments/flute_hamming_distance              tensor(0.0344)\n",
      "instruments/acoustic_guitar_hamming_distance    tensor(0.1781)\n",
      "instruments/electric_guitar_hamming_distance    tensor(0.2961)\n",
      "instruments/organ_hamming_distance              tensor(0.1118)\n",
      "instruments/piano_hamming_distance              tensor(0.4509)\n",
      "instruments/saxophone_hamming_distance          tensor(0.2052)\n",
      "instruments/trumpet_hamming_distance            tensor(0.0971)\n",
      "instruments/violin_hamming_distance             tensor(0.0713)\n",
      "instruments/human_voice_hamming_distance        tensor(0.2654)\n",
      "instruments/cello_f1                                tensor(0.)\n",
      "instruments/clarinet_f1                             tensor(0.)\n",
      "instruments/flute_f1                                tensor(0.)\n",
      "instruments/acoustic_guitar_f1                      tensor(0.)\n",
      "instruments/electric_guitar_f1                      tensor(0.)\n",
      "instruments/organ_f1                                tensor(0.)\n",
      "instruments/piano_f1                                tensor(0.)\n",
      "instruments/saxophone_f1                            tensor(0.)\n",
      "instruments/trumpet_f1                              tensor(0.)\n",
      "instruments/violin_f1                               tensor(0.)\n",
      "instruments/human_voice_f1                      tensor(0.1148)\n",
      "instruments/cello_precision                         tensor(0.)\n",
      "instruments/clarinet_precision                      tensor(0.)\n",
      "instruments/flute_precision                         tensor(0.)\n",
      "instruments/acoustic_guitar_precision               tensor(0.)\n",
      "instruments/electric_guitar_precision               tensor(0.)\n",
      "instruments/organ_precision                         tensor(0.)\n",
      "instruments/piano_precision                         tensor(0.)\n",
      "instruments/saxophone_precision                     tensor(0.)\n",
      "instruments/trumpet_precision                       tensor(0.)\n",
      "instruments/violin_precision                        tensor(0.)\n",
      "instruments/human_voice_precision               tensor(0.9333)\n",
      "instruments/cello_recall                            tensor(0.)\n",
      "instruments/clarinet_recall                         tensor(0.)\n",
      "instruments/flute_recall                            tensor(0.)\n",
      "instruments/acoustic_guitar_recall                  tensor(0.)\n",
      "instruments/electric_guitar_recall                  tensor(0.)\n",
      "instruments/organ_recall                            tensor(0.)\n",
      "instruments/piano_recall                            tensor(0.)\n",
      "instruments/saxophone_recall                        tensor(0.)\n",
      "instruments/trumpet_recall                          tensor(0.)\n",
      "instruments/violin_recall                           tensor(0.)\n",
      "instruments/human_voice_recall                  tensor(0.0611)\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "metrics_raw = get_metrics(y_pred=torch.from_numpy(prediction_raw), y_true=torch.from_numpy(data.y_val), return_per_instrument=True)\n",
    "metrics_raw = pd.Series(metrics_raw)\n",
    "print(metrics_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scaled (normalized) features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X_train example:\n",
      " [-2.74572843  0.85348952 -0.2168907   0.59508776  1.36445458  0.40435308\n",
      "  0.28068384  0.58321878  0.74591997  0.08122014 -0.66495349 -1.40500219\n",
      " -0.94628502 -0.97193183 -0.81476227 -0.11382856  1.38710685  0.51018858\n",
      "  0.53895249  0.68717092  0.22631005 -0.44149987 -0.60745816 -0.79176152\n",
      " -0.07275946 -0.77147255 -0.71967014 -0.36199382] \n",
      "Scaled X_val example:\n",
      " [ 0.46577778  1.14591375 -2.3419525   0.64000228 -0.6452382   1.05630954\n",
      "  1.98016534  0.62292339  0.52668928  0.36654723  0.17209205  0.38449512\n",
      " -0.93832965 -0.79117458 -1.61119927 -0.42495896 -0.60171797 -0.40720068\n",
      " -0.1623136  -0.69332528  1.20649317  3.00454542 -0.80527956 -0.68803475\n",
      "  1.05218384  0.34521036  0.36174927 -0.91609425]\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "X_train_scaled = StandardScaler().fit_transform(data.X_train)\n",
    "X_val_scaled = StandardScaler().fit_transform(data.X_val)\n",
    "\n",
    "print('Scaled X_train example:\\n', X_train_scaled[0], '\\nScaled X_val example:\\n', X_val_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-23 {color: black;background-color: white;}#sk-container-id-23 pre{padding: 0;}#sk-container-id-23 div.sk-toggleable {background-color: white;}#sk-container-id-23 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-23 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-23 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-23 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-23 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-23 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-23 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-23 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-23 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-23 div.sk-item {position: relative;z-index: 1;}#sk-container-id-23 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-23 div.sk-item::before, #sk-container-id-23 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-23 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-23 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-23 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-23 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-23 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-23 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-23 div.sk-label-container {text-align: center;}#sk-container-id-23 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-23 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>multilabelSVM(C=250.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">multilabelSVM</label><div class=\"sk-toggleable__content\"><pre>multilabelSVM(C=250.0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "multilabelSVM(C=250.0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model_norm = multilabelSVM(C=2.5e2, kernel='rbf')\n",
    "model_norm.fit(X=X_train_scaled, y=data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = '.\\src\\model\\SVM\\saved_models\\SVM_model_rbf_norm.sav'\n",
    "pickle.dump(model_norm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few predictions of the model\n",
    "prediction_norm = model_norm.predict(X=X_val_scaled)\n",
    "prediction_norm[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating various metrics for the given model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy                                        tensor(0.8258)\n",
      "hamming_distance                                tensor(0.1742)\n",
      "f1                                              tensor(0.2342)\n",
      "precision                                       tensor(0.3436)\n",
      "recall                                          tensor(0.1855)\n",
      "instruments/cello_accuracy                      tensor(0.9165)\n",
      "instruments/clarinet_accuracy                   tensor(0.9472)\n",
      "instruments/flute_accuracy                      tensor(0.9324)\n",
      "instruments/acoustic_guitar_accuracy            tensor(0.8354)\n",
      "instruments/electric_guitar_accuracy            tensor(0.7420)\n",
      "instruments/organ_accuracy                      tensor(0.8636)\n",
      "instruments/piano_accuracy                      tensor(0.6057)\n",
      "instruments/saxophone_accuracy                  tensor(0.7432)\n",
      "instruments/trumpet_accuracy                    tensor(0.8857)\n",
      "instruments/violin_accuracy                     tensor(0.8550)\n",
      "instruments/human_voice_accuracy                tensor(0.7568)\n",
      "instruments/cello_hamming_distance              tensor(0.0835)\n",
      "instruments/clarinet_hamming_distance           tensor(0.0528)\n",
      "instruments/flute_hamming_distance              tensor(0.0676)\n",
      "instruments/acoustic_guitar_hamming_distance    tensor(0.1646)\n",
      "instruments/electric_guitar_hamming_distance    tensor(0.2580)\n",
      "instruments/organ_hamming_distance              tensor(0.1364)\n",
      "instruments/piano_hamming_distance              tensor(0.3943)\n",
      "instruments/saxophone_hamming_distance          tensor(0.2568)\n",
      "instruments/trumpet_hamming_distance            tensor(0.1143)\n",
      "instruments/violin_hamming_distance             tensor(0.1450)\n",
      "instruments/human_voice_hamming_distance        tensor(0.2432)\n",
      "instruments/cello_f1                            tensor(0.0556)\n",
      "instruments/clarinet_f1                             tensor(0.)\n",
      "instruments/flute_f1                            tensor(0.1538)\n",
      "instruments/acoustic_guitar_f1                  tensor(0.3738)\n",
      "instruments/electric_guitar_f1                  tensor(0.4068)\n",
      "instruments/organ_f1                            tensor(0.2930)\n",
      "instruments/piano_f1                            tensor(0.3462)\n",
      "instruments/saxophone_f1                        tensor(0.2509)\n",
      "instruments/trumpet_f1                          tensor(0.2791)\n",
      "instruments/violin_f1                           tensor(0.0167)\n",
      "instruments/human_voice_f1                      tensor(0.4000)\n",
      "instruments/cello_precision                     tensor(0.0513)\n",
      "instruments/clarinet_precision                      tensor(0.)\n",
      "instruments/flute_precision                     tensor(0.1351)\n",
      "instruments/acoustic_guitar_precision           tensor(0.5797)\n",
      "instruments/electric_guitar_precision           tensor(0.6372)\n",
      "instruments/organ_precision                     tensor(0.3485)\n",
      "instruments/piano_precision                     tensor(0.6855)\n",
      "instruments/saxophone_precision                 tensor(0.3125)\n",
      "instruments/trumpet_precision                   tensor(0.3600)\n",
      "instruments/violin_precision                    tensor(0.0161)\n",
      "instruments/human_voice_precision               tensor(0.6535)\n",
      "instruments/cello_recall                        tensor(0.0606)\n",
      "instruments/clarinet_recall                         tensor(0.)\n",
      "instruments/flute_recall                        tensor(0.1786)\n",
      "instruments/acoustic_guitar_recall              tensor(0.2759)\n",
      "instruments/electric_guitar_recall              tensor(0.2988)\n",
      "instruments/organ_recall                        tensor(0.2527)\n",
      "instruments/piano_recall                        tensor(0.2316)\n",
      "instruments/saxophone_recall                    tensor(0.2096)\n",
      "instruments/trumpet_recall                      tensor(0.2278)\n",
      "instruments/violin_recall                       tensor(0.0172)\n",
      "instruments/human_voice_recall                  tensor(0.2882)\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "metrics_norm = get_metrics(y_pred=torch.from_numpy(prediction_norm), y_true=torch.from_numpy(data.y_val), return_per_instrument=True)\n",
    "metrics_norm = pd.Series(metrics_norm)\n",
    "print(metrics_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24fd04cf1b8dd920425e0015d5188aceeb51298c945c2fa020872a2219f8b4c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
