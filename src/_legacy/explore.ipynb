{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.config.defaults import *\n",
    "\n",
    "from src.utils.utils_functions import *\n",
    "\n",
    "os.chdir(PATH_WORK_DIR)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments_size = {k: 0 for k in INSTRUMENT_TO_IDX.keys()}\n",
    "instrument_dirs = [f.path for f in os.scandir(config.path_irmas_train) if f.is_dir()]\n",
    "\n",
    "for instrument_dir in instrument_dirs:\n",
    "    instrument_name = Path(instrument_dir).stem\n",
    "    num_of_files = len(os.listdir(instrument_dir))\n",
    "    instruments_size[instrument_name] = num_of_files\n",
    "\n",
    "instrument_frequency = sorted(instruments_size.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "plt.figure(figsize=(15, 5), facecolor=\"w\")\n",
    "plt.bar(\n",
    "    [INSTRUMENT_TO_FULLNAME[f[0]] for f in instrument_frequency],\n",
    "    [f[1] for f in instrument_frequency],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import librosa\n",
    "\n",
    "audio_path1 = Path(config.path_irmas_train,\"vio/001__[vio][nod][cou_fol]2194__1.wav\")\n",
    "audio_path2 = Path(config.path_irmas_train,\"gac/[gac][cla]0518__1.wav\")\n",
    "\n",
    "audio1, _ = librosa.load(audio_path1, sr=16_000, mono=True)\n",
    "audio2, _ = librosa.load(audio_path2, sr=16_000, mono=True)\n",
    "\n",
    "audio = [torch.tensor(audio1), torch.tensor(audio2)]\n",
    "\n",
    "audio = torch.vstack(audio)\n",
    "audio.shape\n",
    "\n",
    "spec1 = librosa.feature.melspectrogram(y=audio1, sr=16_000, hop_length=DEFAULT_HOP_LENGTH, n_fft=DEFAULT_N_FFT)\n",
    "spec2 = librosa.feature.melspectrogram(y=audio2, sr=16_000, hop_length=DEFAULT_HOP_LENGTH, n_fft=DEFAULT_N_FFT)\n",
    "spec1.shape\n",
    "specs = torch.stack([torch.tensor(spec1), torch.tensor(spec2)])\n",
    "specs.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.config.defaults import *\n",
    "\n",
    "def spectrogram_batchify(\n",
    "    spectrograms: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Send one or multiple spectrograms and return them in npy form\"\"\"\n",
    "    if isinstance(spectrograms, list):\n",
    "        spectrograms = np.array(spectrograms)\n",
    "    if isinstance(spectrograms, torch.Tensor):\n",
    "        spectrograms = spectrograms.detach().numpy()\n",
    "    if not isinstance(spectrograms, np.ndarray):\n",
    "        assert False, \"Invalid type\"\n",
    "    if len(spectrograms.shape) == 2:\n",
    "        spectrograms = [spectrograms]\n",
    "    elif len(spectrograms.shape) > 3:\n",
    "        assert False, \"spectrograms has to be 1D or 2D (batch)\"\n",
    "    return spectrograms\n",
    "\n",
    "\n",
    "def plot_spectrograms(\n",
    "    spectrograms: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor],\n",
    "    sr=DEFAULT_SAMPLING_RATE,\n",
    "    titles: list[str] | None = None,\n",
    "    type=\"mel\",\n",
    "    hop_length=DEFAULT_HOP_LENGTH,\n",
    "    n_fft=DEFAULT_HOP_LENGTH,\n",
    "):\n",
    "    spectrograms = spectrogram_batchify(spectrograms)\n",
    "    batch_size = len(spectrograms)\n",
    "    if titles is not None and len(titles) != batch_size:\n",
    "        assert False, \"There should be n titles or None\"\n",
    "    sqrt = math.ceil(math.sqrt(batch_size))\n",
    "    n_rows = sqrt\n",
    "    n_cols = sqrt\n",
    "\n",
    "    fig = plt.figure(figsize=(13,9))\n",
    "    for i, spec in enumerate(spectrograms):\n",
    "        title = titles[i] if titles is not None else \"\"\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        print(spec.shape)\n",
    "        spec_db = librosa.power_to_db(spec, ref=np.max)\n",
    "\n",
    "        img = librosa.display.specshow(\n",
    "            spec_db,\n",
    "            y_axis=\"mel\",\n",
    "            x_axis=\"time\",\n",
    "            sr=sr,\n",
    "            hop_length=hop_length,\n",
    "            n_fft=n_fft,\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.colorbar(img, format=\"%+2.f dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def audios_to_mel_spectrograms(\n",
    "    audio: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor],\n",
    "    sr=DEFAULT_SAMPLING_RATE,\n",
    "):\n",
    "    if isinstance(audio, list):\n",
    "        audio = np.array(audio)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.detach().numpy()\n",
    "    if not isinstance(audio, np.ndarray):\n",
    "        assert False, \"Invalid type\"\n",
    "    if len(audio.shape) == 1:\n",
    "        audio = [audio]\n",
    "    elif len(audio.shape) > 2:\n",
    "        assert False, \"Audio has to be 1D or 2D (batch)\"\n",
    "    spectrograms = [librosa.feature.melspectrogram(y=a, sr=sr,hop_length=DEFAULT_HOP_LENGTH, n_fft=DEFAULT_N_FFT) for a in audio]\n",
    "    batched_spectrograms = np.stack(spectrograms)\n",
    "    return batched_spectrograms\n",
    "\n",
    "\n",
    "def audio_melspectrogram_plot(\n",
    "    audio: np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor],\n",
    "    sr=DEFAULT_SAMPLING_RATE,\n",
    "    titles: list[str] | None = None,\n",
    "):\n",
    "    spectrograms = audios_to_mel_spectrograms(audio, sr)\n",
    "    plot_spectrograms(spectrograms, sr=sr, titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_melspectrogram_plot(audio)\n",
    "\n",
    "plot_spectrograms(spectrograms=specs, titles=])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e4a389eac86572b0f73ebe39fcbda7d6e3a0a2775dd0cb233d91e1303dd3463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
